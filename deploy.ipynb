{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8\r\n",
        "# https://medium.com/analytics-vidhya/random-forest-on-titanic-dataset-88327a014b4d\r\n",
        "\r\n",
        "# importing necessary libraries\r\n",
        "from azureml.core import Workspace, Datastore, Dataset, Experiment\r\n",
        "from azureml.data.dataset_factory import DataType\r\n",
        "from azureml.core.model import Model\r\n",
        "from azureml.core.model import InferenceConfig\r\n",
        "from azureml.core.webservice import AciWebservice\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "\r\n",
        "# importing sklearn libraries\r\n",
        "import sklearn\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "# Useful for good split of data into train and test\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# import pandas\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# linear algebra\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# import re package\r\n",
        "import re\r\n",
        "\r\n",
        "# import joblib\r\n",
        "import joblib\r\n",
        "\r\n",
        "# get existing workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "\r\n",
        "# set connection string settings\r\n",
        "subscription_id = 'xxxxxxxx-xxxxxxx-xxxx-xxx-xxxxxxxxx'\r\n",
        "resource_group = 'rg-machinelearning'\r\n",
        "workspace_name = 'machinelearning'\r\n",
        "\r\n",
        "# provide credential information\r\n",
        "blob_datastore_name='machinelearningxxxxxxxxx' # Name of the datastore to workspace\r\n",
        "container_name=os.getenv(\"BLOB_CONTAINER\", \"azureml-blobstore-xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxx\") # Name of Azure blob container\r\n",
        "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"machinelearninxxxxxxxxxxxx\") # Storage account name\r\n",
        "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\") # Storage account access key\r\n",
        "\r\n",
        "# register blob storage account within AMLS\r\n",
        "datastore = Datastore.register_azure_blob_container(workspace=ws, \r\n",
        "                                                         datastore_name=blob_datastore_name, \r\n",
        "                                                         container_name=container_name, \r\n",
        "                                                         account_name=account_name,\r\n",
        "                                                         account_key=account_key,\r\n",
        "                                                         overwrite=True)\r\n",
        "\r\n",
        "# connect to the azure blob storage account\r\n",
        "try:\r\n",
        "    datastore = Datastore.get(ws, blob_datastore_name)\r\n",
        "    print(\"Found Blob Datastore with name: %s\" % blob_datastore_name)\r\n",
        "except UserErrorException:\r\n",
        "    datastore = Datastore.register_azure_blob_container(\r\n",
        "        workspace=ws,\r\n",
        "        datastore_name=blob_datastore_name,\r\n",
        "        account_name=account_name, # Storage account name\r\n",
        "        container_name=container_name, # Name of Azure blob container\r\n",
        "        account_key=account_key,\r\n",
        "        protocol=http) # Storage account key\r\n",
        "    print(\"Registered blob datastore with name: %s\" % blob_datastore_name)\r\n",
        "\r\n",
        "# download all files in local \"download\" folder\r\n",
        "#ds.download(target_path='download', overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "# attach Titanic.csv file\r\n",
        "dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'Titanic.csv')])\r\n",
        "\r\n",
        "# register Dataset as version 1\r\n",
        "dataset.register(workspace = ws, name = 'titanic', create_new_version = True)\r\n",
        "\r\n",
        "# log experiment\r\n",
        "experiment = Experiment(ws, \"TitanicExperiment\")\r\n",
        "run = experiment.start_logging(outputs=None, snapshot_directory=\".\")\r\n",
        "\r\n",
        "# convert dataset to pandas dataframe\r\n",
        "titanic_ds = dataset.to_pandas_dataframe()\r\n",
        "\r\n",
        "# convert ‘Sex’ feature into numeric\r\n",
        "genders = {\"male\": 0, \"female\": 1}\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Sex'] = dataset['Sex'].map(genders)\r\n",
        "\r\n",
        "# since the most common port is Southampton the chances are that the missing one is from there\r\n",
        "titanic_ds['Embarked'].fillna(value='S', inplace=True)\r\n",
        "\r\n",
        "# convert ‘Embarked’ feature into numeric\r\n",
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports)\r\n",
        "\r\n",
        "# convert ‘Survived’ feature into numeric\r\n",
        "ports = {False: 0, True: 1}\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Survived'] = dataset['Survived'].map(ports)\r\n",
        "\r\n",
        "# a cabin number looks like ‘C123’ and the letter refers to the deck.\r\n",
        "# therefore we’re going to extract these and create a new feature, that contains a persons deck. \r\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\r\n",
        "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\r\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\r\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\r\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\r\n",
        "\r\n",
        "# drop cabin since we have a deck feature\r\n",
        "titanic_ds = titanic_ds.drop(['Cabin'], axis=1)\r\n",
        "\r\n",
        "# fix age features missing values\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    mean = titanic_ds[\"Age\"].mean()\r\n",
        "    std = titanic_ds[\"Age\"].std()\r\n",
        "    is_null = dataset[\"Age\"].isnull().sum()\r\n",
        "    # compute random numbers between the mean, std and is_null\r\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\r\n",
        "    # fill NaN values in Age column with random values generated\r\n",
        "    age_slice = dataset[\"Age\"].copy()\r\n",
        "    age_slice[np.isnan(age_slice)] = rand_age\r\n",
        "    dataset[\"Age\"] = age_slice\r\n",
        "    dataset[\"Age\"] = titanic_ds[\"Age\"].astype(int)\r\n",
        "\r\n",
        "# convert ‘age’ to a feature holding a category\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\r\n",
        "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\r\n",
        "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\r\n",
        "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\r\n",
        "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\r\n",
        "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\r\n",
        "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\r\n",
        "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\r\n",
        "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\r\n",
        "\r\n",
        "# create titles\r\n",
        "data = [titanic_ds]\r\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\r\n",
        "\r\n",
        "for dataset in data:\r\n",
        "    # extract titles\r\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\r\n",
        "    # replace titles with a more common title or as Rare\r\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\r\n",
        "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\r\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\r\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\r\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\r\n",
        "    # convert titles into numbers\r\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\r\n",
        "    # filling NaN with 0, to get safe\r\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\r\n",
        "\r\n",
        "# drop name and title column since we have create a title\r\n",
        "titanic_ds = titanic_ds.drop(['Name','Ticket'], axis=1)\r\n",
        "\r\n",
        "# default missing fare rates\r\n",
        "titanic_ds['Fare'].fillna(value=titanic_ds.Fare.mean(), inplace=True)\r\n",
        "\r\n",
        "# convert fare to a feature holding a category\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\r\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\r\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\r\n",
        "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\r\n",
        "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\r\n",
        "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\r\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\r\n",
        "\r\n",
        "# create not_alone and relatives features\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\r\n",
        "    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\r\n",
        "    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\r\n",
        "    dataset['not_alone'] = dataset['not_alone'].astype(int)\r\n",
        "\r\n",
        "# create age class\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\r\n",
        "\r\n",
        "# create fare per person\r\n",
        "data = [titanic_ds]\r\n",
        "for dataset in data:\r\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\r\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\r\n",
        "\r\n",
        "# convert all data to numbers\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "titanic_ds=titanic_ds.apply(le.fit_transform)\r\n",
        "\r\n",
        "print(\"Show first records of all the features created\")\r\n",
        "titanic_ds.head(10)\r\n",
        "\r\n",
        "# convert all data to numbers\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "titanic_ds=titanic_ds.apply(le.fit_transform)\r\n",
        "\r\n",
        "# split our data into a test (30%) and train (70%) dataset\r\n",
        "test_data_split = 0.30\r\n",
        "msk = np.random.rand(len(titanic_ds)) < test_data_split \r\n",
        "test = titanic_ds[msk]\r\n",
        "train = titanic_ds[~msk]\r\n",
        "\r\n",
        "# drop ‘PassengerId’ from the train set, because it does not contribute to a persons survival probability\r\n",
        "train = train.drop(['PassengerId'], axis=1)\r\n",
        "\r\n",
        "# train_test_split is a function to split the dataset X (inputs) and y (output) into X_train,X_test,y_train,y_test respectively.\r\n",
        "# shows 0.2 for testing data, therefore 0.8 for training data. shuffle=True means shuffling data.\r\n",
        "# X_train - This includes your all independent variables,these will be used to train the model, also as we have specified the test_size = 0.4, this means 60% of observations from your complete data will be used to train/fit the model and rest 40% will be used to test the model.\r\n",
        "# X_test - This is remaining 40% portion of the independent variables from the data which will not be used in the training phase and will be used to make predictions to test the accuracy of the model.\r\n",
        "# Y_train - This is your dependent variable which needs to be predicted by this model, this includes category labels against your independent variables, we need to specify our dependent variable while training/fitting the model.\r\n",
        "# Y_test - This data has category labels for your test data, these labels will be used to test the accuracy between actual and predicted categories.\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(train.drop(\"Survived\", axis=1), train[\"Survived\"],test_size=0.4,random_state=54,shuffle=True)\r\n",
        "\r\n",
        "# save data\r\n",
        "np.savetxt('download/train.csv', train, delimiter=',')\r\n",
        "np.savetxt('download/test.csv', test, delimiter=',')\r\n",
        "np.savetxt('download/X_train.csv', X_train, delimiter=',')\r\n",
        "np.savetxt('download/Y_train.csv', X_train, delimiter=',')\r\n",
        "np.savetxt('download/X_test.csv', X_train, delimiter=',')\r\n",
        "np.savetxt('download/Y_test.csv', X_train, delimiter=',')\r\n",
        "\r\n",
        "# upload data to blob storage account\r\n",
        "datastore.upload_files(files=['download/train.csv', 'download/test.csv', 'download/X_train.csv', 'download/Y_train.csv', 'download/X_test.csv', 'download/Y_test.csv'],\r\n",
        "                       target_path='titanic_data/',\r\n",
        "                       overwrite=True)\r\n",
        "\r\n",
        "# attach all datasets\r\n",
        "dataset_train = Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/train.csv')])\r\n",
        "dataset_test = Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/test.csv')])\r\n",
        "dataset_X_train = Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/X_train.csv')])\r\n",
        "dataset_Y_train = Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/Y_train.csv')])\r\n",
        "dataset_X_test = Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/X_test.csv')])\r\n",
        "dataset_Y_test= Dataset.Tabular.from_delimited_files(path=[(datastore, 'titanic_data/Y_test.csv')])\r\n",
        "\r\n",
        "# register datasets as version 1\r\n",
        "dataset_train.register(workspace = ws, name = 'train', create_new_version = True)\r\n",
        "dataset_test.register(workspace = ws, name = 'test', create_new_version = True)\r\n",
        "dataset_X_train.register(workspace = ws, name = 'X_train', create_new_version = True)\r\n",
        "dataset_Y_train.register(workspace = ws, name = 'Y_train', create_new_version = True)\r\n",
        "dataset_X_test.register(workspace = ws, name = 'X_test', create_new_version = True)\r\n",
        "dataset_Y_test.register(workspace = ws, name = 'Y_test', create_new_version = True)\r\n",
        "\r\n",
        "# Random Forest\r\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\r\n",
        "random_forest.fit(X_train, Y_train)\r\n",
        "\r\n",
        "# Save model as pickle file\r\n",
        "joblib.dump(random_forest, \"outputs/random_forest.pkl\")\r\n",
        "\r\n",
        "# Predict and get result\r\n",
        "Y_prediction = random_forest.predict(X_test)\r\n",
        "random_forest.score(X_train, Y_train)\r\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\r\n",
        "\r\n",
        "# register model within workspace\r\n",
        "model_random_forest = Model.register(workspace=ws,\r\n",
        "                       model_name='random_forest',\r\n",
        "                       model_path='outputs/random_forest.pkl',\r\n",
        "                       model_framework=Model.Framework.SCIKITLEARN,\r\n",
        "                       model_framework_version=sklearn.__version__,\r\n",
        "                       sample_input_dataset=dataset_X_train,\r\n",
        "                       sample_output_dataset=dataset_Y_train,\r\n",
        "                       description='Titanic survival prediction using random forest.',\r\n",
        "                       datasets = [('X_train',dataset_X_train),('Y_train',dataset_Y_train),('X_test',dataset_X_test),('Y_test',dataset_Y_test)],\r\n",
        "                       tags={'type': 'regression'})\r\n",
        "\r\n",
        "# create result table showing the outcome of all models\r\n",
        "results = pd.DataFrame({\r\n",
        "    'Model': ['Random Forest'],\r\n",
        "    'Score': [acc_random_forest]})\r\n",
        "result_df = results.sort_values(by='Score', ascending=False)\r\n",
        "result_df = result_df.set_index('Score')\r\n",
        "\r\n",
        "# complete run\r\n",
        "run.log(\"Random Forest accuracy\", acc_random_forest)\r\n",
        "run.complete()\r\n",
        "\r\n",
        "# set service name\r\n",
        "service_name = 'titanic-service'\r\n",
        "service = Model.deploy(ws, service_name, [model_random_forest], overwrite=True)\r\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Blob Datastore with name: machinelearnin9764837687\n",
            "Show first records of all the features created\n",
            "Uploading an estimated of 6 files\n",
            "Uploading download/train.csv\n",
            "Uploaded download/train.csv, 1 files out of an estimated total of 6\n",
            "Uploading download/test.csv\n",
            "Uploaded download/test.csv, 2 files out of an estimated total of 6\n",
            "Uploading download/X_train.csv\n",
            "Uploaded download/X_train.csv, 3 files out of an estimated total of 6\n",
            "Uploading download/Y_train.csv\n",
            "Uploaded download/Y_train.csv, 4 files out of an estimated total of 6\n",
            "Uploading download/X_test.csv\n",
            "Uploaded download/X_test.csv, 5 files out of an estimated total of 6\n",
            "Uploading download/Y_test.csv\n",
            "Uploaded download/Y_test.csv, 6 files out of an estimated total of 6\n",
            "Uploaded 6 files\n",
            "Registering model random_forest\n",
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running\n",
            "2021-09-03 13:31:41+00:00 Creating Container Registry if not exists.\n",
            "2021-09-03 13:31:41+00:00 Registering the environment.\n",
            "2021-09-03 13:31:43+00:00 Uploading autogenerated assets for no-code-deployment.\n",
            "2021-09-03 13:31:45+00:00 Use the existing image.\n",
            "2021-09-03 13:31:46+00:00 Generating deployment configuration.\n",
            "2021-09-03 13:31:46+00:00 Submitting deployment to compute..\n",
            "2021-09-03 13:31:49+00:00 Checking the status of deployment titanic-service..\n",
            "2021-09-03 13:31:59+00:00 Checking the status of inference endpoint titanic-service."
          ]
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1630498332691
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}